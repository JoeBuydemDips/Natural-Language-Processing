{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09378d28",
   "metadata": {},
   "source": [
    "# <center> Code Implementation of BERT </center>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1HWQVIOjmRjaDjDorUUz6qa5JZQobEhYM/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdbfff",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    "- Here is the [[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)](https://arxiv.org/abs/1810.04805) fully detailing what a BERT is; This model was developed by Google AI\n",
    "    - \"BERT is  designed  to  pre-train  deep  bidirectional  representations  from unlabeled text by jointly conditioning on both left  and  right  context  in  all  layers.\" - arXiv:1810.04805v2\n",
    "- BERT is an innovative architecture that lead to substantial improvement in the <b> natural language inference, question answering, sentiment analysis, text summarization, Next Sentence Prediction and so many other fields! </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3a462",
   "metadata": {},
   "source": [
    "# BERT Pro's and its succession to LSTM\n",
    "- You can think of BERT as the successor to LSTM; [Check out my video on LSTM!](https://www.youtube.com/watch?v=rmxogwIjOhE)\n",
    "    - Drawbacks of LSTM's\n",
    "            - Slow to Train\n",
    "                - Considers words in sequential order (not parallel)\n",
    "            - Not \"really\" bi-directional since LSTM has different \"gates\" that executes that logic (but some is lost)\n",
    "- Different BERT models of varying sizes from the paper [linked in this google research github link](https://github.com/google-research/bert)\n",
    "\n",
    "### <center> LSTM Architecture </center>\n",
    "![LSTM](LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa28bd4",
   "metadata": {},
   "source": [
    "# BERT Architecture\n",
    "- The BERT architecture is a multi-layer bidrectional transformer encoder; Here is a [video](https://www.youtube.com/watch?v=X0tB-J8_TS4) and [github link](https://github.com/SpencerPao/Natural-Language-Processing/tree/main/Transformers) on Transformers!\n",
    "    - So it is <b> literally </b> taking the transformer encoder and stacking the encoders on top of each other!\n",
    "        - The BERT base architecture has 12 encoder blocks, 12 multi-attention heads, and 110 million parameters\n",
    "        - The BERT large architecture has 24 enoder blocks, 16 multi-attention heads, and 340 million parameters\n",
    "\n",
    "Image locations came from: <b> [Attention Is All You Need](https://arxiv.org/abs/1706.03762) </b>\n",
    "\n",
    "Encoder Block             |  Attention Framework\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"Encoder.png\" width=\"300\" height=\"450\"> |  <img src=\"Attention.png\" width=\"600\" height=\"900\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98762c6",
   "metadata": {},
   "source": [
    "# How are BERT's Trained?\n",
    "\n",
    "There are two steps in this framework.\n",
    "\n",
    "### Pre-training\n",
    "The BERT model is trained on unblabeled data over different pre-training tasks. This is how the BERT architecture understands the language and context.\n",
    "It accomplishes this in two ways:\n",
    "- Masked Languaged Model (MLM)\n",
    "    - This just masks words and attempts to predict what word would fit in the masked term.\n",
    "    - Original Sentence: \"Make sure to like and subscribe!\"\n",
    "    - Masking: \"Make sure to <b>[MASK1]</b> and <b>[MASK2]</b>\"\n",
    "    - The Model then attempts to predict what the <b>[MASK1]</b> and <b>[MASK2]</b> by plugging in terms.\n",
    "- Next Sentence Prediction (NSP)\n",
    "    - Similarly to the MLM process, the NSP process attempts to predict the next sentence. (and if the next sentence is actually what it is said to be)\n",
    "    - Original Sentences: \n",
    "        - <b> Prior Sentence </b>: \"Make sure to like and subscribe!\"\n",
    "        - <b> Post Sentence </b>: \"I just hit the like button with notificatons and subscribe button!\"\n",
    "    - Does the post sentence follow the prior sentence?\n",
    "    \n",
    "In industry, you will typically utilize an already existing BERT model that has an already pretrained corpus with its distinct vocabulary and either use the model out of the box or go straght to the fine-tuning phase with your training data.\n",
    "\n",
    "# Overview for pre-training:\n",
    "- Train BERT using NSP and MLM\n",
    "    - Every word in sentence returns token embedding\n",
    "    - Incorporate the segment and positional embeddings to account for ordering of inputs\n",
    "    - Pass into BERT\n",
    "    - Outputs word vectors for MLM and a binary value for NSP\n",
    "    - Word vectors passed into a Softmax Layer with X neurons, where X = number of possible words in vocab\n",
    "    - Compare with Cross Entropy Loss, thereby providing prediction for word.\n",
    "<img src=\"Token_Embeddings.png\" width=\"600\" height=\"900\">\n",
    "\n",
    "\n",
    "### Fine-Tuning\n",
    "The BERT model is initialized with the pre-trained parameters and then <b> all </b> the parameters are fine-tuned with labeled data. This is where the model utilizes the underlying understanding of language and context to attempt to solve a problem.\n",
    "- Replace the output layer of BERT with a fully connected network layer where the number of neurons is the number of words for prediction (for QA type problems); the number of neurons can vary among what type of problem you are attempting to solve.\n",
    "- This process is relatively fast to train since the only parameters that will be updated are the output layer parameters\n",
    "    - The other parmaters (encoder blocks) won't change as dramatically\n",
    " \n",
    "\n",
    "\n",
    "### BERT Structure for various NLP tasks\n",
    "- Tok - is a token which is a word.\n",
    "- E - Embeddings (pretrained embeddings from the pre-training step) -- These are vectors of same size\n",
    "- C - Class Labels\n",
    "- CLS - Classification Output (dependent variables : this can be a binary output for example)\n",
    "- T - represents the contextual representation of a token\n",
    "\n",
    "Image locations came from: [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805) and more information on how to approach your specific problem can be addressed there.\n",
    "<img src=\"Fine_Tuned_Tasks.png\" width=\"600\" height=\"900\">\n",
    "\n",
    "# Overview for fine-tuning\n",
    "- Provide supervised dataset and tune the neurons in the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec03c4f",
   "metadata": {},
   "source": [
    "# Cool. Now, let's start applying BERT!\n",
    "There are of course sooo many applications for BERT:\n",
    "- Determine if a movieâ€™s reviews are positive or negative\n",
    "- Help chatbots answer questions\n",
    "- Help predicts text when writing an email\n",
    "- Can quickly summarize long legal contracts\n",
    "\n",
    "Let's keep it simple and see how it can be applied with Sentiment Analysis! Now as you are probably aware, I have done a Sentiment Analysis Video [here](https://www.youtube.com/watch?v=CzRrD76pnVY) but with an LSTM. So, let's do the same with BERT!\n",
    "\n",
    "We want to predict if the text has a POSITIVE or NEGATIVE sentiment associated.\n",
    "- We are going to be conducting Sentiment Analysis.\n",
    "- Please see <b> figure (d) single sentence </b> in the section: BERT Structure for various NLP tasks for architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4627197d",
   "metadata": {},
   "source": [
    "# [Clone Repository](https://github.com/google-research/bert) from Google Research\n",
    "- Pretty much has everything that you need to get started on training and utilizing BERT\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
