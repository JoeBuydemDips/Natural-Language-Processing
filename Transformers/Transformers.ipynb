{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e365f2c8",
   "metadata": {},
   "source": [
    "# <center> What is a [Transformer](https://arxiv.org/abs/1706.03762)? (HIGH LEVEL OVERVIEW) </center>\n",
    "\n",
    "![Transformer](Transformer.png)\n",
    "\n",
    "- The __Transformer__ model that predominatley replaced the [RNN](https://www.youtube.com/watch?v=FBlPZJrJt9g), [GRU](https://www.youtube.com/watch?v=rdz0UqQz5Sw), and [LSTM](https://www.youtube.com/watch?v=rmxogwIjOhE) models due to efficacy and speed. (Links to videos on each of those topics have been covered on my YouTube Channel!\n",
    "- Has the ability to take a more context (getting all the words instead of sequential, iterative input)\n",
    "- Has the ability to highly parallelize the processes\n",
    "- [Paper use-case](https://arxiv.org/abs/1706.03762) was Machine Translation, however, the transformer models (or derivations of) have been used in other cases\n",
    "\n",
    "### Fun fact:\n",
    "- BERT related models uses the Encoder blocks\n",
    "- GPT-3 related models uses the Decoder blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74028691",
   "metadata": {},
   "source": [
    "## What is a **Transformer** Encoder Blocks?\n",
    "\n",
    "<img src=\"Encoder.png\" width=\"340\" height=\"596\">\n",
    "\n",
    "- In layman's terms, the Transformer Blocks' purpose is to associate different weightings in a sentence to identify the most important words that are most critical to the underlying meaning.\n",
    "\n",
    "- __Inputs__: Assigns word embeddings to *each* word in parallel; This is much **faster** than RNN and LSTM architecture.\n",
    "    - You can think of a word embedding as a vector ID in vector space. Each NLP model would typically have different vector values for the same words in english.\n",
    "- __Positional Encoding__: Pass the embeddings and transform with the position encoder, a vector that gives context based on the current position of words\n",
    "    - Provides context to a word by might mean differently in a different setting\n",
    "    - __Embedding of sentence(s) or word(s) + Positional Encoding (Vector Encoding of position in sentence) = Embedding with Context__\n",
    "\n",
    "<img src=\"pos.png\">\n",
    "\n",
    "- pos is the position\n",
    "- i is the dimension\n",
    "- d is the representation dimension\n",
    "\n",
    "### Transformer Encoder Block\n",
    "- Multi-head __Attention__ layer, in laymen's terms, is to identify which word in a sentence is most relevant to other words in the _same_ sentence.\n",
    "    - Each word in a sentence is assigned a probability, where the sum of the probabilities in a sentence is equal to one.\n",
    "        - Example: Make sure to Like and Subscribe!\n",
    "        - Focus on the word __Like__ will have an output vector similar to [0.1, 0.06, 0.02, 0.4, 0.02, 0.4]<sup>T</sup>\n",
    "            - where each probability lines with \"Make sure to Like and Subscribe!\"\n",
    "            - *note* that each focus on a word will result in a different vector. The paper has it such that there are 8 attention vectors that are concatenated, each with length of 64. [original paper on section 3.2.2](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "\n",
    "- __Feed Forward Net__: Transforms the Mult-head Attention output __v__ vectors into a digestable input that can then be used for the next iteration encoder or decoder block. (multi-attention head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142e610d",
   "metadata": {},
   "source": [
    "### Transformer Decoder Block\n",
    "<img src=\"Decoder.png\" width=\"325\" height=\"786\">\n",
    "\n",
    "\n",
    "Instead of your inputs being used as input in this block, you plug in your output into your decoder block.\n",
    "- You do the exact same process of word embeddings and positional vectors as seen in the __Inputs__ and __Positional Encoding__ \n",
    "- There is however a different process called the __Masked Multi-head Attention__ step where there are masked word attributes, meaning that not the entire sentence is taken into consideration but up to 1 to __N__ words are considered (the rest are \"masked\" i.e changed to value of 0.\n",
    "    - Example: Output Vector Text: \"I just liked the video and subscribed!\"\n",
    "    - Masked Vector values [0.05, 0, 0, 0, 0, 0, 0]\n",
    "        - This iterates along ... [0.05, 0.1, 0, 0, 0, 0, 0] etc.\n",
    "        - Note that in a NLP library, the masked values might look like this [MASK]\n",
    "- An additional step after the __Masked Multi-head Attention__ vector step, is that use the output and pass it into another __Multi-head Attention__ block (as seen in the encoder block); however, this is where the \"merging\" of your indpendent and dependent variable vectors meet.\n",
    "    - This determines how related the various input and output vectors are related to each other.\n",
    "        - Returns a vector for each match that represents the relationship betweent the input and output vectors.\n",
    "- We then pass the relationship vector(s) to a feed forward layer and do the typical matrix multiplication\n",
    "    - After this, there could be another decoder block and you repeat the process, OR you calculate the final output for human interpretation\n",
    "\n",
    "### Output Probabilities\n",
    "- The __Linear layer__ has __N__ number of nodes where __N__ is the determined number of outcomes that are possible (This can be as nebulous as total number of words in English) \n",
    "- __Softmax Layer__ converts the output from the Linear Layer to a probability distribution, which is finally interpretable by us and offers a prediction of a word\n",
    "\n",
    "# You keep doing this until the there are no additional words in a sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c71f2",
   "metadata": {},
   "source": [
    "<img src=\"a_lot.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e06dd",
   "metadata": {},
   "source": [
    "## But wait... there's more!\n",
    "# <center> Attention Vector Architecture </center>\n",
    "\n",
    "## Query (Q), Key (K) and Value (V) vectors are trained via backpropagation and start at the same value(s)\n",
    "- These are 3 separate values that are derived from the same vector; linear transformations are applied here\n",
    "    - The proper weights for Q, K and V are learned via training \n",
    "- Think of Q as the actual word embedding\n",
    "- Think of K and V as the memory in the Model -- they could be the same values.\n",
    "\n",
    "The Transformer compares the value of Query (Q) and Key (K) to see where in Key (K) is Q most similar. (like cosine similarity). Then, whichever Key (K) is most similar, return the value (V). This [stackoverflow post](https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms) has a wonderful breakdown on the meaning behind attention values.\n",
    "- Where do the values of K, Q and V come from?\n",
    "    - It *depends*\n",
    "    - If GPT-3, Q, K, and V typically come from the same source (i.e self-attention)\n",
    "    - Machine Translation tasks: Self attention is first applied *then* K and V come from the source sequence and Q comes from the target sequence.\n",
    "\n",
    "## Attention Vector calculation\n",
    "You will have multiple attention vectors per word since V, K and Q represent different components of a word.\n",
    "Finally, the resulting attention vectors are transformed, via weights, to be digestable for the fully connected network.\n",
    "\n",
    "### Steps to calculate Attention\n",
    "    1) Calculate the dot product between the query and key vector of each word. (this is known as the attention score)\n",
    "    2) Divide each of the results by the Square root of the dimension of the key vector (scaled attention score)\n",
    "    3) Pass output from 2) to a softmax function, squeezing the values between 0 and 1\n",
    "    4) Calculate the dot product between the value vectors with the dot product of the softmax output from step 3)\n",
    "    5) Add all weighted value vectors together\n",
    "\n",
    "![multiheadattention](multi_head_attention.png)\n",
    "\n",
    "# <center> Scaled Dot-Product Attention Calculation </center>\n",
    "![Attention_formula](Attention_formula.png)\n",
    "\n",
    "### The Multi-head Attention concatenates the multiple scaled dot-product attention calculations (since there are multiple vectors --- in the paper there are 8 layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a617eed8",
   "metadata": {},
   "source": [
    "# Phew!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6cf4e7",
   "metadata": {},
   "source": [
    "# That was a lot.\n",
    "# So, how would we go about implementing this architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5cb024",
   "metadata": {},
   "source": [
    "# [Here is an excellent notebook that details the paper step by step in python code](http://nlp.seas.harvard.edu/2018/04/03/attention.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac759eda",
   "metadata": {},
   "source": [
    "# Here is a great [github repository](https://github.com/jadore801120/attention-is-all-you-need-pytorch) that more or less implemented the paper and is downloadable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
